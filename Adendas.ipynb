{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2674e5a9-9066-4605-be4f-78459409cf93",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"ICSARA (SEIA) — Extracción heurística desde PDF a JSON\n",
    "\n",
    "Descripción:\n",
    "Proceso heurístico desarrollado para transformar un ICSARA en formato PDF en un dataset\n",
    "estructurado, trazable y reutilizable. El sistema no se basa en marcadores formales del\n",
    "documento, sino que **determina** capítulos, bisagras, preguntas y elementos gráficos a\n",
    "partir de patrones de layout, tipografía, posición relativa y continuidad entre páginas.\n",
    "\n",
    "Flujo:\n",
    "  1. Abrir el PDF una única vez y recorrerlo secuencialmente por página.\n",
    "  2. Determinar tablas vectoriales y figuras raster mediante heurísticas de layout\n",
    "     (detección por bounding boxes y características gráficas).\n",
    "  3. Recortar y exportar cada tabla/figura a PNG con nomenclatura estable:\n",
    "       p{numero_pregunta}_parte{N}_{tabla|figura}.png\n",
    "  4. Extraer texto plano excluyendo heurísticamente las áreas ocupadas por tablas y figuras,\n",
    "     evitando duplicación de contenido y ruido visual.\n",
    "  5. Determinar capítulos mediante heurísticas tipográficas (romanos en negrita, tamaño de\n",
    "     fuente, posición) y resolver continuidad entre páginas (cross-page).\n",
    "  6. Determinar bisagras mediante patrones de layout y semántica superficial, asociándolas\n",
    "     a las preguntas correspondientes.\n",
    "  7. Limpiar heurísticamente artefactos no textuales: firmas digitales, encabezados\n",
    "     repetidos y bisagras residuales.\n",
    "  8. Consolidar cada pregunta en una estructura consistente con su capítulo, bisagra,\n",
    "     texto limpio y referencias a tablas/figuras asociadas.\n",
    "\n",
    "Salidas:\n",
    "  - preguntas.json\n",
    "      Dataset estructurado por pregunta:\n",
    "      {capitulo, bisagra, numero, texto,\n",
    "       tablas_figuras:[{tipo, parte, png}]}\n",
    "\n",
    "  - preguntas.txt\n",
    "      Representación legible y continua del contenido textual.\n",
    "\n",
    "  - outputs_png/\n",
    "      Recortes de tablas y figuras con trazabilidad directa a cada pregunta.\n",
    "\n",
    "  - chapters_hinges.json\n",
    "      Archivo de apoyo para validación y depuración de las heurísticas de detección.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from bisect import bisect_right\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import fitz  # pymupdf\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "BASE_DIR = Path(os.getenv(\"ICSARA_BASE_DIR\", str(Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd())))\n",
    "PDF_PATH = Path(os.getenv(\"ICSARA_PDF_PATH\", str(BASE_DIR / \"1766432953_2167380849.pdf\")))\n",
    "OUT_DIR = Path(os.getenv(\"ICSARA_OUT_DIR\", str(BASE_DIR / \"salida_icsara\")))\n",
    "SEPARADOR_PREGUNTA = \"------------\"\n",
    "\n",
    "# --- Texto ---\n",
    "PAT_PREGUNTA = re.compile(r\"(?m)^\\s*(\\d{1,4})\\.\\s+\")\n",
    "FRASES_RUIDO = [\n",
    "    \"Para validar las firmas de este documento\",\n",
    "    \"sea.gob.cl/validar\", \"validar las firmas\",\n",
    "    \"https://validador.sea.gob.cl/validar\",\n",
    "    \"Firmado Digitalmente\", \"sellodigital.sea.gob.cl\",\n",
    "    \"Razón:\", \"Razon:\",\n",
    "]\n",
    "FIRMA_TOKENS = (\n",
    "    \"firmado digitalmente\", \"sellodigital\", \"utc\",\n",
    "    \"fecha:\", \"razón\", \"razon\", \"lugar:\",\n",
    ")\n",
    "MIN_RATIO_FRECUENTES = 0.60\n",
    "YEAR_MIN, YEAR_MAX = 1900, 2100\n",
    "MONO_DROP_THRESHOLD = 5\n",
    "\n",
    "RE_CAP_ROM_TIT = re.compile(r\"^\\s*([IVXLCDM]{1,10})\\.\\s+(.+?)\\s*$\", re.IGNORECASE)\n",
    "RE_CAP_ROM_SOLO = re.compile(r\"^\\s*([IVXLCDM]{1,10})\\.\\s*$\", re.IGNORECASE)\n",
    "RE_NUM_SOLO = re.compile(r\"^\\s*\\d{1,4}\\.\\s*$\")\n",
    "\n",
    "RE_TABLA_PARTES = re.compile(r\"(?i)^\\s*Tabla\\s+XX\\.\\s*Partes\\s+y\\s+obras\\s+del\\s+Proyecto\\s*$\")\n",
    "RE_NOMBRE_PARTE = re.compile(r\"^\\s*\\[(Nombre\\s+parte/obra\\s+.+?)\\]\\s*$\", re.IGNORECASE)\n",
    "RE_CARACTER = re.compile(r\"^\\s*\\[(Temporal\\s+o\\s+permanente)\\]\\s*$\", re.IGNORECASE)\n",
    "RE_FASE = re.compile(r\"^\\s*\\[(Construcción.*?cierre)\\]\\s*$\", re.IGNORECASE)\n",
    "\n",
    "RE_FIRMA_BLOQUE = re.compile(\n",
    "    r\"(?:Fecha:\\s*\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\\s+\\d{1,2}:\\d{2}[:\\d.]*\\s*(?:UTC\\s*[+-]?\\d{2}:\\d{2})?\\s*(?:Lugar:\\s*)?)+\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "RE_FIRMA_COMPLETA = re.compile(r\"(?:Firmado\\s+Digitalmente\\s+por\\s+.+?)(?=\\n|$)\", re.IGNORECASE)\n",
    "RE_FECHA_PRE_FIRMA = re.compile(r\"\\d{1,2}\\s+de\\s+\\w+\\s+de\\s+\\d{4}\\.\\s*$\", re.IGNORECASE)\n",
    "\n",
    "# --- Detección tablas/figuras ---\n",
    "THIN_MAX = 1.2\n",
    "MIN_LINE_LEN = 25.0\n",
    "MIN_HLINES = 6\n",
    "MIN_VLINES = 4\n",
    "MERGE_GAP = 12.0\n",
    "MIN_TABLE_AREA = 15_000.0\n",
    "MIN_FIG_AREA = 8_000.0\n",
    "PNG_DPI = 200\n",
    "PNG_DIRNAME = \"outputs_png\"\n",
    "\n",
    "# --- Layout ---\n",
    "SAME_LINE_Y = 2.5\n",
    "SAME_LINE_X_GAP = 22.0\n",
    "Y_GAP_MERGE = 6.0\n",
    "X_TOL_MERGE = 24.0\n",
    "MAX_BISAGRA_TO_Q_GAP = 55.0\n",
    "BOTTOM_PAGE_MARGIN = 120.0\n",
    "TOP_NEXT_PAGE_SEARCH = 250.0\n",
    "\n",
    "RE_ROMAN = re.compile(r\"^\\s*([IVXLCDM]{1,10})\\.\\s+(.+?)\\s*$\", re.IGNORECASE)\n",
    "RE_QSTART = re.compile(r\"^\\s*(\\d{1,4})\\.\\s+\")\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  GEOMETRÍA\n",
    "# #############################################################################\n",
    "def rect_area(r):\n",
    "    return max(0.0, (r.x1 - r.x0)) * max(0.0, (r.y1 - r.y0))\n",
    "\n",
    "def union_rect(a, b):\n",
    "    return fitz.Rect(min(a.x0, b.x0), min(a.y0, b.y0), max(a.x1, b.x1), max(a.y1, b.y1))\n",
    "\n",
    "def intersects(a, b):\n",
    "    return a.intersects(b)\n",
    "\n",
    "def rect_close(a, b, gap):\n",
    "    dx = max(0.0, max(a.x0 - b.x1, b.x0 - a.x1))\n",
    "    dy = max(0.0, max(a.y0 - b.y1, b.y0 - a.y1))\n",
    "    return dx <= gap and dy <= gap\n",
    "\n",
    "def merge_rects(rects, gap=MERGE_GAP):\n",
    "    rects = [fitz.Rect(r) for r in rects]\n",
    "    out = []\n",
    "    for r in rects:\n",
    "        merged = False\n",
    "        for i in range(len(out)):\n",
    "            if rect_close(out[i], r, gap):\n",
    "                out[i] = union_rect(out[i], r)\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            out.append(r)\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        new_out = []\n",
    "        while out:\n",
    "            r = out.pop()\n",
    "            merged_any = False\n",
    "            for i in range(len(out)):\n",
    "                if rect_close(out[i], r, gap):\n",
    "                    out[i] = union_rect(out[i], r)\n",
    "                    merged_any = True\n",
    "                    changed = True\n",
    "                    break\n",
    "            if not merged_any:\n",
    "                new_out.append(r)\n",
    "        out = new_out\n",
    "    return out\n",
    "\n",
    "def in_any_rect(point_y0, rects):\n",
    "    \"\"\"Verifica si una coordenada y0 cae dentro de algún rect.\"\"\"\n",
    "    for r in rects:\n",
    "        if r.y0 <= point_y0 <= r.y1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  DETECCIÓN TABLAS VECTORIALES Y FIGURAS RASTER\n",
    "# #############################################################################\n",
    "def extract_table_candidates(page):\n",
    "    drawings = page.get_drawings()\n",
    "    h_lines, v_lines = [], []\n",
    "    for d in drawings:\n",
    "        for it in d.get(\"items\", []):\n",
    "            op = it[0]\n",
    "            if op == \"l\":\n",
    "                (x1, y1), (x2, y2) = it[1], it[2]\n",
    "                dx, dy = abs(x2 - x1), abs(y2 - y1)\n",
    "                if dx >= MIN_LINE_LEN and dy <= 1.0:\n",
    "                    h_lines.append(fitz.Rect(min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)))\n",
    "                elif dy >= MIN_LINE_LEN and dx <= 1.0:\n",
    "                    v_lines.append(fitz.Rect(min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)))\n",
    "            elif op == \"re\":\n",
    "                r = fitz.Rect(it[1])\n",
    "                w, h = abs(r.x1 - r.x0), abs(r.y1 - r.y0)\n",
    "                if h <= THIN_MAX and w >= MIN_LINE_LEN:\n",
    "                    h_lines.append(r)\n",
    "                elif w <= THIN_MAX and h >= MIN_LINE_LEN:\n",
    "                    v_lines.append(r)\n",
    "    if len(h_lines) < MIN_HLINES or len(v_lines) < MIN_VLINES:\n",
    "        return []\n",
    "    all_rects = h_lines + v_lines\n",
    "    bbox = all_rects[0]\n",
    "    for r in all_rects[1:]:\n",
    "        bbox = union_rect(bbox, r)\n",
    "    if rect_area(bbox) < MIN_TABLE_AREA:\n",
    "        return []\n",
    "    merged = merge_rects(all_rects, gap=MERGE_GAP)\n",
    "    groups = merge_rects(merged, gap=MERGE_GAP * 2)\n",
    "    return [g for g in groups if rect_area(g) >= MIN_TABLE_AREA]\n",
    "\n",
    "\n",
    "def extract_raster_figures(page):\n",
    "    figs = []\n",
    "    for img in page.get_images(full=True):\n",
    "        xref = img[0]\n",
    "        for r in page.get_image_rects(xref):\n",
    "            rr = fitz.Rect(r)\n",
    "            if rect_area(rr) >= MIN_FIG_AREA:\n",
    "                figs.append(rr)\n",
    "    return merge_rects(figs, gap=10.0)\n",
    "\n",
    "\n",
    "def save_bbox_screenshot(doc, page_index0, bbox, out_dir, fname, dpi=PNG_DPI):\n",
    "    page = doc[page_index0]\n",
    "    zoom = dpi / 72.0\n",
    "    mat = fitz.Matrix(zoom, zoom)\n",
    "    pix = page.get_pixmap(matrix=mat, clip=fitz.Rect(bbox), alpha=False)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = out_dir / fname\n",
    "    pix.save(out_path.as_posix())\n",
    "    return out_path.as_posix()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  TEXTO PLANO — EXCLUYENDO ZONAS DE TABLAS/FIGURAS\n",
    "# #############################################################################\n",
    "def extract_page_text_excluding_bboxes(page, exclude_rects):\n",
    "    \"\"\"\n",
    "    Extrae texto de la página excluyendo las zonas de tablas/figuras.\n",
    "    Usa page.get_text(\"dict\") y filtra bloques/líneas cuyos spans\n",
    "    caigan dentro de algún rect excluido.\n",
    "    \"\"\"\n",
    "    d = page.get_text(\"dict\")\n",
    "    out_lines = []\n",
    "\n",
    "    for block in d.get(\"blocks\", []):\n",
    "        if block.get(\"type\") != 0:\n",
    "            continue\n",
    "        for line in block.get(\"lines\", []):\n",
    "            line_bbox = fitz.Rect(line[\"bbox\"])\n",
    "            # Si la línea intersecta con algún rect excluido, omitirla\n",
    "            skip = False\n",
    "            for er in exclude_rects:\n",
    "                if intersects(line_bbox, er):\n",
    "                    skip = True\n",
    "                    break\n",
    "            if skip:\n",
    "                continue\n",
    "            # Reconstruir texto de la línea\n",
    "            text = \"\".join(sp.get(\"text\", \"\") for sp in line.get(\"spans\", []))\n",
    "            out_lines.append(text)\n",
    "\n",
    "    return \"\\n\".join(out_lines)\n",
    "\n",
    "\n",
    "def normalize_lines_keep_empty(text):\n",
    "    return [ln.rstrip(\"\\r\") for ln in text.replace(\"\\x0c\", \"\\n\").split(\"\\n\")]\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  FILTRO HEADERS/FOOTERS + STITCH\n",
    "# #############################################################################\n",
    "def build_frequent_line_filter(pages_lines, min_ratio=MIN_RATIO_FRECUENTES):\n",
    "    n = len(pages_lines)\n",
    "    c = Counter()\n",
    "    for lines in pages_lines:\n",
    "        for ln in set(ln.strip() for ln in lines if ln.strip()):\n",
    "            c[ln] += 1\n",
    "    threshold = max(2, int(n * min_ratio))\n",
    "    return {ln for ln, k in c.items() if k >= threshold}\n",
    "\n",
    "\n",
    "def clean_page_lines_keep_empty(lines, frequent_lines):\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        s = ln.strip()\n",
    "        if s == \"\":\n",
    "            out.append(\"\")\n",
    "            continue\n",
    "        if s in frequent_lines:\n",
    "            continue\n",
    "        low = s.lower()\n",
    "        if any(fr.lower() in low for fr in FRASES_RUIDO):\n",
    "            continue\n",
    "        if re.fullmatch(r\"\\d{1,4}\", s):\n",
    "            continue\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "\n",
    "def stitch_pages(pages_clean_lines):\n",
    "    texto = \"\"\n",
    "    for lines in pages_clean_lines:\n",
    "        page_text = \"\\n\".join(lines).strip()\n",
    "        if not page_text:\n",
    "            continue\n",
    "        if not texto:\n",
    "            texto = page_text\n",
    "            continue\n",
    "        prev = texto.rstrip()\n",
    "        if prev.endswith(\"-\"):\n",
    "            texto = prev[:-1] + page_text.lstrip()\n",
    "        else:\n",
    "            texto += \"\\n\" + page_text.lstrip()\n",
    "    return texto\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  NORMALIZACIÓN Y FORMATEO DE TEXTO\n",
    "# #############################################################################\n",
    "def normalize_text_preserving_paragraphs(text):\n",
    "    text = re.sub(r\"-\\n(?=\\w)\", \"\", text)\n",
    "    text = re.sub(r\"\\n\\s*\\n+\", \"\\n<<<PARA>>>\\n\", text)\n",
    "    list_start = r\"(?:[A-Za-z]\\)|\\d+\\)|\\([A-Za-z0-9]+\\)|[-•])\"\n",
    "    text = re.sub(rf\"\\n\\s*(?={list_start})\", \"\\n<<<PARA>>>\\n\", text)\n",
    "    text = re.sub(r\"\\n+\", \" \", text)\n",
    "    text = text.replace(\"<<<PARA>>>\", \"\\n\")\n",
    "    text = re.sub(r\"[ \\t]{2,}\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def looks_table_row_horizontal(line):\n",
    "    if re.search(r\"\\s{2,}\", line.strip()):\n",
    "        cols = [c for c in re.split(r\"\\s{2,}\", line.strip()) if c.strip()]\n",
    "        return len(cols) >= 2\n",
    "    return False\n",
    "\n",
    "\n",
    "def split_table_row_horizontal(line):\n",
    "    return [re.sub(r\"\\s{2,}\", \" \", c.strip()) for c in re.split(r\"\\s{2,}\", line.strip()) if c.strip()]\n",
    "\n",
    "\n",
    "def format_horizontal_table_as_semicolon(lines):\n",
    "    rows = [split_table_row_horizontal(ln) for ln in lines if ln.strip()]\n",
    "    if not rows:\n",
    "        return \"\"\n",
    "    mx = max(len(r) for r in rows)\n",
    "    rows = [r + [\"\"] * (mx - len(r)) for r in rows]\n",
    "    return \"\\n\".join(\";\".join(r) for r in rows)\n",
    "\n",
    "\n",
    "def parse_tabla_partes_obras(lines, start_idx):\n",
    "    i = start_idx\n",
    "    if i >= len(lines) or not RE_TABLA_PARTES.match(lines[i].strip()):\n",
    "        return \"\", start_idx\n",
    "    table_title = lines[i].strip()\n",
    "    i += 1\n",
    "    while i < len(lines) and not RE_NOMBRE_PARTE.match(lines[i].strip()):\n",
    "        i += 1\n",
    "    rows = []\n",
    "    header = [\"Tabla\", \"Nombre\", \"Descripción\", \"Carácter\", \"Fase\"]\n",
    "    while i < len(lines):\n",
    "        if RE_TABLA_PARTES.match(lines[i].strip()):\n",
    "            i += 1\n",
    "            while i < len(lines) and not RE_NOMBRE_PARTE.match(lines[i].strip()):\n",
    "                i += 1\n",
    "            continue\n",
    "        m_name = RE_NOMBRE_PARTE.match(lines[i].strip())\n",
    "        if not m_name:\n",
    "            break\n",
    "        nombre = f\"[{m_name.group(1)}]\"\n",
    "        i += 1\n",
    "        desc_lines = []\n",
    "        while i < len(lines) and not RE_CARACTER.match(lines[i].strip()):\n",
    "            if RE_NOMBRE_PARTE.match(lines[i].strip()):\n",
    "                break\n",
    "            desc_lines.append(lines[i])\n",
    "            i += 1\n",
    "        descripcion = normalize_text_preserving_paragraphs(\"\\n\".join(desc_lines)).strip()\n",
    "        caracter = \"\"\n",
    "        if i < len(lines) and RE_CARACTER.match(lines[i].strip()):\n",
    "            caracter = \"Temporal o permanente\"\n",
    "            i += 1\n",
    "        fase = \"\"\n",
    "        if i < len(lines):\n",
    "            m_f = RE_FASE.match(lines[i].strip())\n",
    "            if m_f:\n",
    "                fase = m_f.group(1)\n",
    "                i += 1\n",
    "            elif lines[i].strip().startswith(\"[\") and lines[i].strip().endswith(\"]\"):\n",
    "                fase = lines[i].strip().strip(\"[]\").strip()\n",
    "                i += 1\n",
    "        rows.append([table_title, nombre, descripcion, caracter, fase])\n",
    "        while i < len(lines) and lines[i].strip() == \"\":\n",
    "            i += 1\n",
    "        if i < len(lines) and RE_NUM_SOLO.match(lines[i]):\n",
    "            break\n",
    "    if not rows:\n",
    "        return \"\", start_idx\n",
    "    out_lines = [\";\".join(header)]\n",
    "    for r in rows:\n",
    "        out_lines.append(\";\".join(r))\n",
    "    return \"\\n\".join(out_lines), i\n",
    "\n",
    "\n",
    "def format_question(texto_raw):\n",
    "    lines = normalize_lines_keep_empty(texto_raw)\n",
    "    out_parts = []\n",
    "    buf_text = []\n",
    "\n",
    "    def flush_text():\n",
    "        nonlocal buf_text\n",
    "        if not buf_text:\n",
    "            return\n",
    "        block = \"\\n\".join(buf_text).strip()\n",
    "        if block:\n",
    "            out_parts.append(normalize_text_preserving_paragraphs(block))\n",
    "        buf_text = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(lines):\n",
    "        ln = lines[i]\n",
    "        if ln.strip() == \"\":\n",
    "            buf_text.append(\"\")\n",
    "            i += 1\n",
    "            continue\n",
    "        if RE_TABLA_PARTES.match(ln.strip()):\n",
    "            flush_text()\n",
    "            table_csv, new_i = parse_tabla_partes_obras(lines, i)\n",
    "            if table_csv:\n",
    "                out_parts.append(table_csv)\n",
    "                i = new_i\n",
    "                continue\n",
    "        if looks_table_row_horizontal(ln):\n",
    "            flush_text()\n",
    "            table_lines = []\n",
    "            while i < len(lines) and lines[i].strip() != \"\" and looks_table_row_horizontal(lines[i]):\n",
    "                table_lines.append(lines[i])\n",
    "                i += 1\n",
    "            table_txt = format_horizontal_table_as_semicolon(table_lines)\n",
    "            if table_txt:\n",
    "                out_parts.append(table_txt)\n",
    "            continue\n",
    "        buf_text.append(ln)\n",
    "        i += 1\n",
    "    flush_text()\n",
    "    return \"\\n\".join(p for p in out_parts if p).strip()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  LIMPIEZA POST-FORMATO\n",
    "# #############################################################################\n",
    "def clean_firma_digital(texto):\n",
    "    texto = RE_FIRMA_BLOQUE.sub(\"\", texto)\n",
    "    texto = RE_FIRMA_COMPLETA.sub(\"\", texto)\n",
    "    texto = texto.rstrip()\n",
    "    m = RE_FECHA_PRE_FIRMA.search(texto)\n",
    "    if m:\n",
    "        pos = m.start()\n",
    "        remaining = RE_FECHA_PRE_FIRMA.sub(\"\", texto[pos:]).strip()\n",
    "        if not remaining:\n",
    "            texto = texto[:pos].rstrip()\n",
    "    return re.sub(r\"[\\s\\n]+$\", \"\", texto)\n",
    "\n",
    "\n",
    "def clean_trailing_hinge(texto, all_hinge_texts):\n",
    "    if not all_hinge_texts:\n",
    "        return texto\n",
    "    ts = texto.rstrip()\n",
    "    for ht in all_hinge_texts:\n",
    "        ht = ht.strip()\n",
    "        if not ht:\n",
    "            continue\n",
    "        if ts.endswith(ht):\n",
    "            ts = ts[:-len(ht)].rstrip()\n",
    "            break\n",
    "        if ht.endswith(\".\") and ts.endswith(ht[:-1]):\n",
    "            ts = ts[:-(len(ht) - 1)].rstrip()\n",
    "            break\n",
    "    return re.sub(r\"[\\s\\n]+$\", \"\", ts)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  PREGUNTAS — DETECCIÓN DESDE TEXTO PLANO\n",
    "# #############################################################################\n",
    "def get_line_bounds(text, pos):\n",
    "    ls = text.rfind(\"\\n\", 0, pos) + 1\n",
    "    le = text.find(\"\\n\", pos)\n",
    "    return ls, (le if le != -1 else len(text))\n",
    "\n",
    "\n",
    "def next_nonempty_line(text, start_pos):\n",
    "    i = start_pos\n",
    "    n = len(text)\n",
    "    while i < n:\n",
    "        j = text.find(\"\\n\", i)\n",
    "        if j == -1:\n",
    "            return text[i:].strip()\n",
    "        line = text[i:j].strip()\n",
    "        if line:\n",
    "            return line\n",
    "        i = j + 1\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def is_false_question_start(texto_total, match_start, num_str):\n",
    "    ls, le = get_line_bounds(texto_total, match_start)\n",
    "    line = texto_total[ls:le].strip()\n",
    "    low = line.lower()\n",
    "    if len(num_str) == 4:\n",
    "        try:\n",
    "            n = int(num_str)\n",
    "            if YEAR_MIN <= n <= YEAR_MAX and any(t in low for t in FIRMA_TOKENS):\n",
    "                return True\n",
    "        except: pass\n",
    "    only_num = bool(re.fullmatch(r\"\\s*\" + re.escape(num_str) + r\"\\.\\s*\", line))\n",
    "    if not only_num:\n",
    "        return False\n",
    "    if len(num_str) == 4:\n",
    "        try:\n",
    "            n = int(num_str)\n",
    "            if YEAR_MIN <= n <= YEAR_MAX:\n",
    "                return True\n",
    "        except: pass\n",
    "    if len(num_str) >= 2 and num_str[0] == \"0\":\n",
    "        return True\n",
    "    nxt = next_nonempty_line(texto_total, le + 1)\n",
    "    if nxt and not re.match(r\"[A-Za-zÁÉÍÓÚÜÑáéíóúüñ]\", nxt[0]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def apply_monotonic_filter(starts):\n",
    "    kept = []\n",
    "    last_num = None\n",
    "    for ini, num in starts:\n",
    "        try: n = int(num)\n",
    "        except: continue\n",
    "        if last_num is None:\n",
    "            kept.append((ini, num)); last_num = n; continue\n",
    "        if n < last_num and (last_num - n) >= MONO_DROP_THRESHOLD:\n",
    "            continue\n",
    "        kept.append((ini, num)); last_num = n\n",
    "    return kept\n",
    "\n",
    "\n",
    "def extract_questions_from_text(texto_total):\n",
    "    matches = list(PAT_PREGUNTA.finditer(texto_total))\n",
    "    starts = []\n",
    "    for m in matches:\n",
    "        ini, num = m.start(), m.group(1)\n",
    "        if not is_false_question_start(texto_total, ini, num):\n",
    "            starts.append((ini, num))\n",
    "    starts = apply_monotonic_filter(starts)\n",
    "    out = []\n",
    "    for i, (ini, num) in enumerate(starts):\n",
    "        fin = starts[i + 1][0] if i + 1 < len(starts) else len(texto_total)\n",
    "        raw_sin = re.sub(rf\"^\\s*{re.escape(num)}\\.\\s*\", \"\", texto_total[ini:fin].strip(), count=1)\n",
    "        out.append({\"numero\": int(num), \"texto\": format_question(raw_sin)})\n",
    "    return out\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  LAYOUT — CAPÍTULOS Y BISAGRAS\n",
    "# #############################################################################\n",
    "def extract_spans(page):\n",
    "    d = page.get_text(\"dict\")\n",
    "    spans = []\n",
    "    for block in d.get(\"blocks\", []):\n",
    "        if block.get(\"type\") != 0: continue\n",
    "        for line in block.get(\"lines\", []):\n",
    "            for sp in line.get(\"spans\", []):\n",
    "                txt = (sp.get(\"text\") or \"\").strip()\n",
    "                if not txt: continue\n",
    "                bbox = fitz.Rect(sp[\"bbox\"])\n",
    "                flags = int(sp.get(\"flags\", 0))\n",
    "                font = (sp.get(\"font\") or \"\").lower()\n",
    "                is_bold = (\"bold\" in font) or (flags & 16)\n",
    "                spans.append({\"text\": txt, \"bbox\": bbox, \"is_bold\": bool(is_bold)})\n",
    "    return spans\n",
    "\n",
    "\n",
    "def build_lines_from_spans(spans):\n",
    "    spans = sorted(spans, key=lambda s: (round(s[\"bbox\"].y0, 1), s[\"bbox\"].x0))\n",
    "    lines = []\n",
    "    for sp in spans:\n",
    "        r = sp[\"bbox\"]\n",
    "        placed = False\n",
    "        for ln in lines:\n",
    "            if abs(ln[\"_y0\"] - r.y0) <= SAME_LINE_Y:\n",
    "                ln[\"spans\"].append(sp)\n",
    "                ln[\"_y0\"] = (ln[\"_y0\"] + r.y0) / 2.0\n",
    "                ln[\"_bbox\"] = union_rect(ln[\"_bbox\"], r)\n",
    "                placed = True; break\n",
    "        if not placed:\n",
    "            lines.append({\"_y0\": r.y0, \"_bbox\": fitz.Rect(r), \"spans\": [sp]})\n",
    "    out = []\n",
    "    for ln in lines:\n",
    "        sps = sorted(ln[\"spans\"], key=lambda s: s[\"bbox\"].x0)\n",
    "        parts = []; prev = None\n",
    "        for sp in sps:\n",
    "            if prev is None:\n",
    "                parts.append(sp[\"text\"])\n",
    "            else:\n",
    "                gap = sp[\"bbox\"].x0 - prev[\"bbox\"].x1\n",
    "                if gap > SAME_LINE_X_GAP:\n",
    "                    parts.append(\" \" + sp[\"text\"])\n",
    "                elif parts and not parts[-1].endswith((\" \", \"-\", \"\\u201c\", \"\\\"\", \"(\", \"/\")) \\\n",
    "                     and not sp[\"text\"].startswith((\",\", \".\", \")\", \":\", \";\")):\n",
    "                    parts.append(\" \" + sp[\"text\"])\n",
    "                else:\n",
    "                    parts.append(sp[\"text\"])\n",
    "            prev = sp\n",
    "        text = \"\".join(parts).strip()\n",
    "        bold_count = sum(1 for sp in sps if sp[\"is_bold\"])\n",
    "        out.append({\"text\": text, \"bbox\": ln[\"_bbox\"], \"spans\": sps,\n",
    "                     \"is_bold_line\": bold_count / max(1, len(sps)) >= 0.6})\n",
    "    out.sort(key=lambda x: (x[\"bbox\"].y0, x[\"bbox\"].x0))\n",
    "    return out\n",
    "\n",
    "\n",
    "def merge_bold_lines(bold_lines):\n",
    "    if not bold_lines: return []\n",
    "    bold_lines = sorted(bold_lines, key=lambda x: (x[\"bbox\"].y0, x[\"bbox\"].x0))\n",
    "    merged = []; cur = None\n",
    "    for ln in bold_lines:\n",
    "        if cur is None:\n",
    "            cur = {\"text\": ln[\"text\"], \"bbox\": fitz.Rect(ln[\"bbox\"])}; continue\n",
    "        dy = ln[\"bbox\"].y0 - cur[\"bbox\"].y1\n",
    "        same = abs(ln[\"bbox\"].y0 - cur[\"bbox\"].y0) <= SAME_LINE_Y and ln[\"bbox\"].x0 >= cur[\"bbox\"].x0\n",
    "        nxt = (0.0 <= dy <= Y_GAP_MERGE) and abs(ln[\"bbox\"].x0 - cur[\"bbox\"].x0) <= X_TOL_MERGE\n",
    "        if same or nxt:\n",
    "            cur[\"text\"] = (cur[\"text\"] + \" \" + ln[\"text\"]).strip()\n",
    "            cur[\"bbox\"] = union_rect(cur[\"bbox\"], ln[\"bbox\"])\n",
    "        else:\n",
    "            merged.append(cur); cur = {\"text\": ln[\"text\"], \"bbox\": fitz.Rect(ln[\"bbox\"])}\n",
    "    if cur: merged.append(cur)\n",
    "    for m in merged: m[\"text\"] = re.sub(r\"\\s{2,}\", \" \", m[\"text\"]).strip()\n",
    "    return merged\n",
    "\n",
    "\n",
    "def detect_qstarts_layout(lines, exclude_rects, page_no):\n",
    "    q = []\n",
    "    for ln in lines:\n",
    "        skip = False\n",
    "        for er in exclude_rects:\n",
    "            if intersects(ln[\"bbox\"], er):\n",
    "                skip = True; break\n",
    "        if skip: continue\n",
    "        m = RE_QSTART.match(ln[\"text\"])\n",
    "        if m:\n",
    "            q.append({\"num\": int(m.group(1)), \"bbox\": ln[\"bbox\"], \"text\": ln[\"text\"]})\n",
    "    q.sort(key=lambda x: (x[\"bbox\"].y0, x[\"bbox\"].x0))\n",
    "    return q\n",
    "\n",
    "\n",
    "def classify_bolds(merged_bolds, qstarts, exclude_rects, page_no, page_height, next_qstarts=None):\n",
    "    chapters, hinges = [], []\n",
    "    for b in merged_bolds:\n",
    "        skip = False\n",
    "        for er in exclude_rects:\n",
    "            if intersects(b[\"bbox\"], er):\n",
    "                skip = True; break\n",
    "        if skip: continue\n",
    "        txt = b[\"text\"].strip()\n",
    "        if RE_ROMAN.match(txt):\n",
    "            chapters.append({\"type\": \"chapter\", \"page\": page_no, \"text\": txt,\n",
    "                             \"bbox\": [b[\"bbox\"].x0, b[\"bbox\"].y0, b[\"bbox\"].x1, b[\"bbox\"].y1],\n",
    "                             \"sort_key\": (page_no, b[\"bbox\"].y0)})\n",
    "            continue\n",
    "        b_bottom = b[\"bbox\"].y1\n",
    "        cand = None\n",
    "        for qs in qstarts:\n",
    "            if qs[\"bbox\"].y0 < b_bottom: continue\n",
    "            gap = qs[\"bbox\"].y0 - b_bottom\n",
    "            if gap <= MAX_BISAGRA_TO_Q_GAP: cand = qs; break\n",
    "            if gap > MAX_BISAGRA_TO_Q_GAP: break\n",
    "        if cand is None and next_qstarts and (page_height - b_bottom) <= BOTTOM_PAGE_MARGIN:\n",
    "            for qs in next_qstarts:\n",
    "                if qs[\"bbox\"].y0 <= TOP_NEXT_PAGE_SEARCH: cand = qs; break\n",
    "        if cand is not None:\n",
    "            hinges.append({\"type\": \"hinge\", \"page\": page_no, \"text\": txt,\n",
    "                           \"bbox\": [b[\"bbox\"].x0, b[\"bbox\"].y0, b[\"bbox\"].x1, b[\"bbox\"].y1],\n",
    "                           \"sort_key\": (page_no, b[\"bbox\"].y0)})\n",
    "    return chapters, hinges\n",
    "\n",
    "\n",
    "def filter_questions_by_continuity(all_qs):\n",
    "    if not all_qs: return []\n",
    "    sorted_qs = sorted(all_qs, key=lambda q: q[\"sort_key\"])\n",
    "    filtered = []; last = -1\n",
    "    for q in sorted_qs:\n",
    "        if last < 0 or q[\"num\"] >= last:\n",
    "            filtered.append(q); last = q[\"num\"]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def build_hierarchy(chapters, hinges, all_questions):\n",
    "    timeline = []\n",
    "    for ch in chapters: timeline.append({\"kind\": \"chapter\", \"sort_key\": ch[\"sort_key\"], \"data\": ch})\n",
    "    for h in hinges:   timeline.append({\"kind\": \"hinge\",   \"sort_key\": h[\"sort_key\"],  \"data\": h})\n",
    "    for q in all_questions: timeline.append({\"kind\": \"question\", \"sort_key\": q[\"sort_key\"], \"data\": q})\n",
    "    timeline.sort(key=lambda x: x[\"sort_key\"])\n",
    "\n",
    "    result = []; cur_ch = None; cur_h = None\n",
    "\n",
    "    def fin_h():\n",
    "        nonlocal cur_h\n",
    "        if cur_h and cur_ch: cur_ch[\"hinges\"].append(cur_h)\n",
    "        cur_h = None\n",
    "\n",
    "    def fin_ch():\n",
    "        nonlocal cur_ch, cur_h\n",
    "        fin_h()\n",
    "        if cur_ch: result.append(cur_ch)\n",
    "        cur_ch = None\n",
    "\n",
    "    for item in timeline:\n",
    "        k = item[\"kind\"]\n",
    "        if k == \"chapter\":\n",
    "            fin_ch()\n",
    "            d = item[\"data\"]\n",
    "            cur_ch = {\"page\": d[\"page\"], \"text\": d[\"text\"], \"bbox\": d[\"bbox\"],\n",
    "                      \"questions\": [], \"hinges\": [], \"questions_without_hinge\": []}\n",
    "            cur_h = None\n",
    "        elif k == \"hinge\":\n",
    "            if not cur_ch: continue\n",
    "            fin_h()\n",
    "            d = item[\"data\"]\n",
    "            cur_h = {\"page\": d[\"page\"], \"text\": d[\"text\"], \"bbox\": d[\"bbox\"], \"questions\": []}\n",
    "        elif k == \"question\":\n",
    "            qn = item[\"data\"][\"num\"]\n",
    "            if cur_ch:\n",
    "                cur_ch[\"questions\"].append(qn)\n",
    "                if cur_h: cur_h[\"questions\"].append(qn)\n",
    "                else: cur_ch[\"questions_without_hinge\"].append(qn)\n",
    "    fin_ch()\n",
    "    return result\n",
    "\n",
    "\n",
    "def build_question_lookup(hierarchy):\n",
    "    lookup = {}\n",
    "    for ch in hierarchy:\n",
    "        for h in ch[\"hinges\"]:\n",
    "            for qn in h[\"questions\"]:\n",
    "                lookup[qn] = {\"capitulo\": ch[\"text\"], \"bisagra\": h[\"text\"]}\n",
    "        for qn in ch[\"questions_without_hinge\"]:\n",
    "            lookup[qn] = {\"capitulo\": ch[\"text\"], \"bisagra\": None}\n",
    "    return lookup\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  ASOCIAR TABLAS/FIGURAS → PREGUNTA (por posición)\n",
    "# #############################################################################\n",
    "def find_parent_question(sort_keys, qs_sorted, page, y0):\n",
    "    idx = bisect_right(sort_keys, (page, y0)) - 1\n",
    "    return qs_sorted[idx][\"num\"] if idx >= 0 else None\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  SALIDA\n",
    "# #############################################################################\n",
    "def save_outputs(out_dir, texto_total, preguntas_final, hierarchy):\n",
    "    outp = Path(out_dir)\n",
    "    outp.mkdir(parents=True, exist_ok=True)\n",
    "    (outp / \"texto_total.txt\").write_text(texto_total, encoding=\"utf-8\")\n",
    "    (outp / \"preguntas.json\").write_text(json.dumps(preguntas_final, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    with (outp / \"preguntas.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for p in preguntas_final:\n",
    "            f.write(SEPARADOR_PREGUNTA + \"\\n\")\n",
    "            if p[\"capitulo\"]: f.write(f\"CAPITULO: {p['capitulo']}\\n\")\n",
    "            if p[\"bisagra\"]:  f.write(f\"BISAGRA: {p['bisagra']}\\n\")\n",
    "            f.write(f\"NUMERO: {p['numero']}\\n\")\n",
    "            f.write(p[\"texto\"] + \"\\n\")\n",
    "            if p[\"tablas_figuras\"]:\n",
    "                for tf in p[\"tablas_figuras\"]:\n",
    "                    f.write(f\"  [{tf['tipo'].upper()}] parte {tf['parte']}: {tf['png']}\\n\")\n",
    "        f.write(SEPARADOR_PREGUNTA + \"\\n\")\n",
    "    (outp / \"chapters_hinges.json\").write_text(\n",
    "        json.dumps({\"chapters\": hierarchy}, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "#  MAIN\n",
    "# #############################################################################\n",
    "def main():\n",
    "    if not PDF_PATH.exists():\n",
    "        print(f\"No se encontro el PDF: {PDF_PATH}\")\n",
    "        return\n",
    "\n",
    "    doc = fitz.open(str(PDF_PATH))\n",
    "    outp = Path(OUT_DIR)\n",
    "    png_dir = outp / PNG_DIRNAME\n",
    "\n",
    "    total_pages = len(doc)\n",
    "\n",
    "    # =========================================================================\n",
    "    # FASE 1: Detectar tablas/figuras + exportar PNGs + extraer texto sin tablas\n",
    "    # =========================================================================\n",
    "    all_detections = []           # lista global de detecciones\n",
    "    pages_text_clean = []         # texto por página (sin tablas/figuras)\n",
    "    all_page_layout_data = []     # datos de layout por página\n",
    "    exclude_by_page = {}          # {page_no: [Rect, ...]}\n",
    "\n",
    "    for pno in range(total_pages):\n",
    "        page = doc[pno]\n",
    "        page_no = pno + 1\n",
    "\n",
    "        # Detectar bboxes a excluir\n",
    "        tables = extract_table_candidates(page)\n",
    "        figs = extract_raster_figures(page)\n",
    "        excludes = tables + figs\n",
    "        exclude_by_page[page_no] = excludes\n",
    "\n",
    "        # Registrar detecciones (sin nombre aún, se asigna en Fase 2)\n",
    "        for r in tables:\n",
    "            all_detections.append({\n",
    "                \"tipo\": \"tabla\", \"page\": page_no, \"page_idx\": pno,\n",
    "                \"bbox\": r, \"pregunta\": None, \"parte\": None, \"png\": None,\n",
    "            })\n",
    "        for r in figs:\n",
    "            all_detections.append({\n",
    "                \"tipo\": \"figura\", \"page\": page_no, \"page_idx\": pno,\n",
    "                \"bbox\": r, \"pregunta\": None, \"parte\": None, \"png\": None,\n",
    "            })\n",
    "\n",
    "        # Texto de la página excluyendo tablas/figuras\n",
    "        page_text = extract_page_text_excluding_bboxes(page, excludes)\n",
    "        pages_text_clean.append(page_text)\n",
    "\n",
    "        # Layout data para Módulo B\n",
    "        spans = extract_spans(page)\n",
    "        lines = build_lines_from_spans(spans)\n",
    "        bold_lines = [ln for ln in lines if ln[\"is_bold_line\"] and ln[\"text\"]]\n",
    "        merged_bolds = merge_bold_lines(bold_lines)\n",
    "        qstarts = detect_qstarts_layout(lines, excludes, page_no)\n",
    "\n",
    "        all_page_layout_data.append({\n",
    "            \"page_no\": page_no, \"page_height\": page.rect.height,\n",
    "            \"merged_bolds\": merged_bolds, \"qstarts\": qstarts,\n",
    "        })\n",
    "\n",
    "    # =========================================================================\n",
    "    # FASE 2: Texto plano → preguntas\n",
    "    # =========================================================================\n",
    "    pages_lines = [normalize_lines_keep_empty(t) for t in pages_text_clean]\n",
    "    frequent_lines = build_frequent_line_filter(pages_lines)\n",
    "    pages_clean_lines = [clean_page_lines_keep_empty(lines, frequent_lines) for lines in pages_lines]\n",
    "    texto_total = stitch_pages(pages_clean_lines)\n",
    "    preguntas_text = extract_questions_from_text(texto_total)\n",
    "\n",
    "    # =========================================================================\n",
    "    # FASE 3: Layout → capítulos, bisagras, jerarquía\n",
    "    # =========================================================================\n",
    "    all_chapters, all_hinges = [], []\n",
    "    for i, pd in enumerate(all_page_layout_data):\n",
    "        next_qs = all_page_layout_data[i + 1][\"qstarts\"] if i + 1 < len(all_page_layout_data) else None\n",
    "        excludes = exclude_by_page.get(pd[\"page_no\"], [])\n",
    "        chs, hgs = classify_bolds(\n",
    "            pd[\"merged_bolds\"], pd[\"qstarts\"], excludes,\n",
    "            pd[\"page_no\"], pd[\"page_height\"], next_qstarts=next_qs)\n",
    "        all_chapters.extend(chs)\n",
    "        all_hinges.extend(hgs)\n",
    "\n",
    "    all_qs_raw = []\n",
    "    for pd in all_page_layout_data:\n",
    "        for qs in pd[\"qstarts\"]:\n",
    "            all_qs_raw.append({\"num\": qs[\"num\"], \"page\": pd[\"page_no\"],\n",
    "                               \"sort_key\": (pd[\"page_no\"], qs[\"bbox\"].y0)})\n",
    "    all_qs_filtered = filter_questions_by_continuity(all_qs_raw)\n",
    "    hierarchy = build_hierarchy(all_chapters, all_hinges, all_qs_filtered)\n",
    "    lookup = build_question_lookup(hierarchy)\n",
    "\n",
    "    # =========================================================================\n",
    "    # FASE 4: Asociar detecciones → preguntas + exportar PNGs\n",
    "    # =========================================================================\n",
    "    sort_keys = [(q[\"page\"], q[\"sort_key\"][1]) for q in sorted(all_qs_filtered, key=lambda q: q[\"sort_key\"])]\n",
    "    qs_sorted = sorted(all_qs_filtered, key=lambda q: q[\"sort_key\"])\n",
    "\n",
    "    part_counters = defaultdict(int)\n",
    "\n",
    "    for det in all_detections:\n",
    "        parent_q = find_parent_question(sort_keys, qs_sorted, det[\"page\"], det[\"bbox\"].y0)\n",
    "        det[\"pregunta\"] = parent_q\n",
    "        q_label = f\"{parent_q:03d}\" if parent_q is not None else \"000\"\n",
    "        part_counters[(q_label, det[\"tipo\"])] += 1\n",
    "        parte = part_counters[(q_label, det[\"tipo\"])]\n",
    "        det[\"parte\"] = parte\n",
    "\n",
    "        fname = f\"p{q_label}_parte{parte:03d}_{det['tipo']}.png\"\n",
    "        png_path = save_bbox_screenshot(doc, det[\"page_idx\"], det[\"bbox\"], png_dir, fname)\n",
    "        det[\"png\"] = fname  # solo nombre, no ruta completa\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    # =========================================================================\n",
    "    # FASE 5: Cruzar preguntas + limpiezas + asociar tablas/figuras\n",
    "    # =========================================================================\n",
    "    all_hinge_texts = []\n",
    "    for ch in hierarchy:\n",
    "        for h in ch[\"hinges\"]:\n",
    "            all_hinge_texts.append(h[\"text\"])\n",
    "\n",
    "    # Agrupar detecciones por pregunta\n",
    "    dets_by_q = defaultdict(list)\n",
    "    for det in all_detections:\n",
    "        if det[\"pregunta\"] is not None:\n",
    "            dets_by_q[det[\"pregunta\"]].append({\n",
    "                \"tipo\": det[\"tipo\"], \"parte\": det[\"parte\"], \"png\": det[\"png\"],\n",
    "            })\n",
    "\n",
    "    preguntas_final = []\n",
    "    for p in preguntas_text:\n",
    "        num = p[\"numero\"]\n",
    "        info = lookup.get(num, {})\n",
    "        texto = p[\"texto\"]\n",
    "        texto = clean_firma_digital(texto)\n",
    "        texto = clean_trailing_hinge(texto, all_hinge_texts)\n",
    "\n",
    "        tf_list = sorted(dets_by_q.get(num, []), key=lambda x: (x[\"tipo\"], x[\"parte\"]))\n",
    "\n",
    "        preguntas_final.append({\n",
    "            \"capitulo\": info.get(\"capitulo\", \"\"),\n",
    "            \"bisagra\": info.get(\"bisagra\"),\n",
    "            \"numero\": num,\n",
    "            \"texto\": texto,\n",
    "            \"tablas_figuras\": tf_list,\n",
    "        })\n",
    "\n",
    "    # =========================================================================\n",
    "    # GUARDAR\n",
    "    # =========================================================================\n",
    "    save_outputs(outp, texto_total, preguntas_final, hierarchy)\n",
    "\n",
    "    # =========================================================================\n",
    "    # REPORTE\n",
    "    # =========================================================================\n",
    "    n_caps = len(hierarchy)\n",
    "    n_bis = sum(len(ch[\"hinges\"]) for ch in hierarchy)\n",
    "    n_preg = len(preguntas_final)\n",
    "    n_det = len(all_detections)\n",
    "    n_tab = sum(1 for d in all_detections if d[\"tipo\"] == \"tabla\")\n",
    "    n_fig = sum(1 for d in all_detections if d[\"tipo\"] == \"figura\")\n",
    "\n",
    "    print(f\"PDF: {PDF_PATH}\")\n",
    "    print(f\"Páginas: {total_pages}\")\n",
    "    print(f\"Capítulos: {n_caps} | Bisagras: {n_bis} | Preguntas: {n_preg}\")\n",
    "    print(f\"Tablas: {n_tab} | Figuras: {n_fig} | Total detecciones: {n_det}\")\n",
    "    print(f\"PNGs en: {png_dir}\")\n",
    "    print(f\"Salida en: {outp}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1844b-0b07-44e8-bb5f-636076e66708",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Clasificador temático de preguntas ICSARA (SEIA Chile)\n",
    "\n",
    "Este script lee un archivo preguntas.json y, para cada pregunta, calcula afinidad temática\n",
    "contra una taxonomía (temas + keywords). El cálculo usa coincidencias de keywords en tres\n",
    "zonas del texto: capítulo, bisagra y texto principal, asignando pesos distintos por zona.\n",
    "\n",
    "Para cada pregunta:\n",
    "- Normaliza los textos para robustecer coincidencias (minúsculas, espacios y una versión sin acentos).\n",
    "- Evalúa cada tema: detecta keywords presentes y suma puntaje según dónde aparece cada keyword.\n",
    "- Filtra temas bajo un umbral mínimo para reducir falsos positivos.\n",
    "- Ordena los temas detectados por score descendente.\n",
    "- Devuelve una clasificación estructurada con:\n",
    "  - tema_principal / tema_principal_id (tema con mayor score)\n",
    "  - temas_principales / temas_principales_id (conjunto de temas que cumplen criterio de principal)\n",
    "  - temas_secundarios (los siguientes temas más relevantes)\n",
    "  - keywords_match (evidencia: keywords encontradas)\n",
    "  - detalle por zona (capítulo/bisagra/texto) para trazabilidad\n",
    "\n",
    "Finalmente exporta preguntas_clasificadas.json con los campos originales más las etiquetas,\n",
    "scores y evidencias de coincidencia.\n",
    "\n",
    "Uso:\n",
    "  python clasificar_preguntas.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIG\n",
    "# =============================================================================\n",
    "BASE_DIR = Path(os.getenv(\"ICSARA_BASE_DIR\", str(Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd())))\n",
    "DEFAULT_OUT_DIR = Path(os.getenv(\"ICSARA_OUT_DIR\", str(BASE_DIR / \"salida_icsara\")))\n",
    "INPUT_JSON = Path(os.getenv(\"ICSARA_INPUT_JSON\", str(DEFAULT_OUT_DIR / \"preguntas.json\")))\n",
    "OUTPUT_JSON = Path(os.getenv(\"ICSARA_OUTPUT_JSON\", str(DEFAULT_OUT_DIR / \"preguntas_clasificadas.json\")))\n",
    "OUTPUT_JSON_DETALLE = Path(os.getenv(\"ICSARA_OUTPUT_JSON_DETALLE\", str(DEFAULT_OUT_DIR / \"preguntas_clasificadas_detalle.json\")))\n",
    "\n",
    "# Umbral mínimo de score para asignar un tema (evitar falsos positivos)\n",
    "MIN_SCORE = 2\n",
    "\n",
    "# Peso por zona de coincidencia\n",
    "PESO_CAPITULO = 3.0\n",
    "PESO_BISAGRA = 5.0\n",
    "PESO_TEXTO = 1.0\n",
    "\n",
    "# Regla de multi-principal:\n",
    "# Todo tema con score >= max(MIN_SCORE, MULTI_PRINCIPAL_RATIO * top_score) se marca como principal\n",
    "MULTI_PRINCIPAL_RATIO = 0.80\n",
    "\n",
    "# =============================================================================\n",
    "# TAXONOMÍA ICSARA — DICCIONARIO COMPLETO (con 2 temas nuevos)\n",
    "# =============================================================================\n",
    "TAXONOMIA = {\n",
    "    \"CALIDAD_AIRE\": {\n",
    "        \"nombre\": \"Calidad del Aire y Emisiones Atmosféricas\",\n",
    "        \"keywords\": [\n",
    "            \"mp10\", \"mp2,5\", \"mp2.5\", \"pm10\", \"pm2.5\", \"pm2,5\",\n",
    "            \"material particulado\", \"so2\", \"so₂\", \"dióxido de azufre\",\n",
    "            \"no2\", \"no₂\", \"nox\", \"dióxido de nitrógeno\", \"óxidos de nitrógeno\",\n",
    "            \"monóxido de carbono\", \"ozono troposférico\", \"plomo atmosférico\",\n",
    "            \"benceno\", \"compuestos orgánicos volátiles\", \"cov\",\n",
    "            \"emisiones atmosféricas\", \"emisiones fugitivas\",\n",
    "            \"calpuff\", \"aermod\", \"screen3\", \"wrf\", \"calmet\",\n",
    "            \"modelación de dispersión\", \"modelo de dispersión\",\n",
    "            \"isopletas\", \"rosa de vientos\", \"estabilidad atmosférica\",\n",
    "            \"fuente fija\", \"fuente difusa\", \"fuente móvil\", \"chimenea\",\n",
    "            \"factor de emisión\", \"ap-42\", \"cems\",\n",
    "            \"inventario de emisiones\",\n",
    "            \"ds 12/2022\", \"ds 12/2010\", \"ds 104/2018\", \"ds 114/2002\",\n",
    "            \"ds 115/2002\", \"ds 112/2002\", \"ds 136/2000\", \"ds 05/2023\",\n",
    "            \"ds 13/2011\", \"ds 28/2013\", \"ds 29/2013\", \"ds 4/1992\",\n",
    "            \"ds 37/2013\", \"ds 09/2023\", \"ds 138/2005\", \"ds 144/1961\",\n",
    "            \"norma de calidad del aire\", \"norma de emisión atmosférica\",\n",
    "            \"zona latente\", \"zona saturada\", \"ppda\",\n",
    "            \"plan de prevención y descontaminación\",\n",
    "            \"sinca\", \"calidad del aire\",\n",
    "            \"percentil 98\", \"concentración 24h\", \"promedio anual\",\n",
    "            \"estación de monitoreo de calidad del aire\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"RUIDO_VIBRACIONES\": {\n",
    "        \"nombre\": \"Ruido y Vibraciones\",\n",
    "        \"keywords\": [\n",
    "            \"ruido\", \"vibraciones\", \"vibración\",\n",
    "            \"nivel de presión sonora\", \"nps\", \"npseq\", \"npc\",\n",
    "            \"decibel\", \"db(a)\", \"dba\",\n",
    "            \"horario diurno\", \"horario nocturno\",\n",
    "            \"ruido de fondo\", \"ruido residual\",\n",
    "            \"barrera acústica\", \"pantalla acústica\",\n",
    "            \"propagación sonora\", \"modelo acústico\",\n",
    "            \"ruido tonal\", \"ruido impulsivo\",\n",
    "            \"ruido submarino\", \"efecto corona\",\n",
    "            \"ds 38/2011\", \"ds 38\", \"ds 146/1997\", \"ds 146\",\n",
    "            \"ne-01\", \"ne-02\",\n",
    "            \"zona i\", \"zona ii\", \"zona iii\", \"zona iv\",\n",
    "            \"nch 1619\",\n",
    "            \"predicción y evaluación de impactos por ruido\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"GEOLOGIA_SUELOS\": {\n",
    "        \"nombre\": \"Geología, Geomorfología y Suelos\",\n",
    "        \"keywords\": [\n",
    "            \"geología\", \"geomorfología\", \"suelo\", \"suelos\",\n",
    "            \"capacidad de uso\", \"clase de uso\",\n",
    "            \"taxonomía usda\", \"serie de suelos\",\n",
    "            \"perfil edáfico\", \"horizontes del suelo\",\n",
    "            \"textura del suelo\", \"erosión\",\n",
    "            \"erosión hídrica\", \"erosión eólica\", \"cárcavas\",\n",
    "            \"aptitud agrícola\", \"aptitud forestal\", \"aptitud ganadera\",\n",
    "            \"remoción en masa\", \"deslizamiento\", \"subsidencia\",\n",
    "            \"sismicidad\", \"falla geológica\", \"riesgo geológico\",\n",
    "            \"ciren\", \"sernageomin\",\n",
    "            \"dl 3.557\", \"dl 3557\",\n",
    "            \"contaminación de suelos\",\n",
    "            \"permeabilidad del suelo\", \"pedregosidad\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"RECURSO_HIDRICO\": {\n",
    "        \"nombre\": \"Recurso Hídrico, Hidrología e Hidrogeología\",\n",
    "        \"keywords\": [\n",
    "            \"recurso hídrico\", \"recursos hídricos\",\n",
    "            \"hidrología\", \"hidrogeología\", \"hidrogeológico\",\n",
    "            \"caudal ecológico\", \"caudal mínimo ecológico\", \"caudal ambiental\",\n",
    "            \"derechos de aprovechamiento de aguas\",\n",
    "            \"balance hídrico\", \"cuenca hidrográfica\",\n",
    "            \"acuífero\", \"zona de recarga\", \"zona de descarga\",\n",
    "            \"nivel freático\", \"piezometría\", \"conductividad hidráulica\",\n",
    "            \"aguas subterráneas\", \"aguas superficiales\",\n",
    "            \"calidad de aguas\", \"cuerpo receptor\",\n",
    "            \"riles\", \"ptas\", \"planta de tratamiento de aguas\",\n",
    "            \"dbo\", \"dqo\", \"sst\", \"nkt\",\n",
    "            \"coliformes fecales\", \"coliformes totales\",\n",
    "            \"curva de duración de caudales\",\n",
    "            \"modflow\", \"modelo hidrogeológico\",\n",
    "            \"nch 1333\", \"nch 409\",\n",
    "            \"ds 90/2000\", \"ds 90\", \"ds 46/2002\", \"ds 46\",\n",
    "            \"código de aguas\", \"dfl 1.122\",\n",
    "            \"norma secundaria de calidad de aguas\",\n",
    "            \"dga\", \"dirección general de aguas\",\n",
    "            \"doh\", \"dirección de obras hidráulicas\",\n",
    "            \"siss\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"GLACIARES\": {\n",
    "        \"nombre\": \"Glaciares y Criósfera\",\n",
    "        \"keywords\": [\n",
    "            \"glaciar\", \"glaciares\", \"glaciaretes\",\n",
    "            \"glaciar rocoso\", \"glaciares rocosos\",\n",
    "            \"criósfera\", \"permafrost\",\n",
    "            \"balance de masa glaciar\", \"retroceso glaciar\",\n",
    "            \"zona periglaciar\",\n",
    "            \"inventario de glaciares\",\n",
    "            \"unidad de glaciología\",\n",
    "            \"estrategia nacional de glaciares\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"FLORA_VEGETACION\": {\n",
    "        \"nombre\": \"Flora y Vegetación\",\n",
    "        \"keywords\": [\n",
    "            \"flora\", \"vegetación\", \"vegetal\",\n",
    "            \"inventario florístico\", \"catastro vegetacional\",\n",
    "            \"formaciones vegetacionales\", \"formaciones xerofíticas\",\n",
    "            \"unidades homogéneas de vegetación\", \"uhv\",\n",
    "            \"cobertura vegetal\",\n",
    "            \"flora leñosa\", \"flora no leñosa\", \"suculentas\",\n",
    "            \"matorral\", \"bosque esclerófilo\", \"bosque nativo\",\n",
    "            \"bosque de preservación\",\n",
    "            \"categoría de conservación\", \"rce\",\n",
    "            \"especie en categoría\", \"clasificación de especies\",\n",
    "            \"monumento natural\", \"formación relictual\",\n",
    "            \"fotointerpretación\",\n",
    "            \"singularidades ambientales\",\n",
    "            \"revegetación\", \"reforestación\", \"restauración ecológica\",\n",
    "            \"ley 20.283\", \"bosque nativo\",\n",
    "            \"ds 68/2009\", \"dl 701\", \"ds 4.363\",\n",
    "            \"pas 148\", \"pas 149\", \"pas 150\", \"pas 151\",\n",
    "            \"pas 152\", \"pas 153\",\n",
    "            \"artículo 148\", \"artículo 149\", \"artículo 150\",\n",
    "            \"artículo 151\", \"artículo 152\", \"artículo 153\",\n",
    "            \"pas 127\", \"pas 128\", \"pas 129\",\n",
    "            \"conaf\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"FAUNA\": {\n",
    "        \"nombre\": \"Fauna Terrestre\",\n",
    "        \"keywords\": [\n",
    "            \"fauna\", \"fauna silvestre\", \"fauna terrestre\",\n",
    "            \"ensamble faunístico\",\n",
    "            \"categoría de conservación fauna\",\n",
    "            \"herpetofauna\", \"mastofauna\", \"avifauna\", \"entomofauna\",\n",
    "            \"aves\", \"mamíferos\", \"reptiles\", \"anfibios\",\n",
    "            \"murciélagos\", \"quirópteros\",\n",
    "            \"trampa sherman\", \"trampa tomahawk\", \"trampa de foso\", \"pitfall\",\n",
    "            \"cámara trampa\", \"fotomonitoreo\",\n",
    "            \"red de niebla\", \"redes de niebla\",\n",
    "            \"transecto lineal\", \"punto de conteo\",\n",
    "            \"censo visual\", \"censo auditivo\",\n",
    "            \"rescate y relocalización\", \"relocalización de fauna\",\n",
    "            \"corredor biológico\", \"endemismo\",\n",
    "            \"plan de manejo de fauna\", \"perturbación controlada\",\n",
    "            \"nidificación\", \"migración\", \"reproducción fauna\",\n",
    "            \"riqueza de especies\", \"diversidad shannon\", \"diversidad simpson\",\n",
    "            \"ley 19.473\", \"ley de caza\", \"ds 5/1998\", \"cites\",\n",
    "            \"pas 146\", \"pas 147\", \"pas 123\", \"pas 124\",\n",
    "            \"artículo 146\", \"artículo 147\",\n",
    "            \"sag\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"ECOSISTEMAS_ACUATICOS\": {\n",
    "        \"nombre\": \"Ecosistemas Acuáticos Continentales y Humedales\",\n",
    "        \"keywords\": [\n",
    "            \"ecosistema acuático\", \"ecosistemas acuáticos\",\n",
    "            \"limnología\", \"limnológico\",\n",
    "            \"fauna íctica\", \"ictiofauna\", \"peces continentales\",\n",
    "            \"macroinvertebrados bentónicos\", \"macroinvertebrados\",\n",
    "            \"fitoplancton\", \"zooplancton\", \"macrófitas\",\n",
    "            \"índice biótico\", \"ibf\", \"ept\", \"bmwp\",\n",
    "            \"hábitat acuático\", \"ribereño\",\n",
    "            \"humedal\", \"humedales\", \"bofedal\", \"bofedales\",\n",
    "            \"vega\", \"vegas\", \"turbera\", \"turberas\",\n",
    "            \"humedal urbano\", \"humedales urbanos\",\n",
    "            \"inventario nacional de humedales\", \"ramsar\",\n",
    "            \"ley 21.202\",\n",
    "            \"ds 15 mma\",\n",
    "            \"pas 155\", \"pas 156\", \"pas 157\", \"pas 158\", \"pas 159\",\n",
    "            \"artículo 155\", \"artículo 156\", \"artículo 157\",\n",
    "            \"artículo 158\", \"artículo 159\",\n",
    "            \"modificación de cauce\", \"modificaciones de cauce\",\n",
    "            \"obra hidráulica\", \"obras hidráulicas\",\n",
    "            \"regularización de cauce\", \"defensa de cauce\",\n",
    "            \"extracción de ripio\", \"extracción de arena\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"ECOSISTEMAS_MARINOS\": {\n",
    "        \"nombre\": \"Ecosistemas Marinos\",\n",
    "        \"keywords\": [\n",
    "            \"ecosistema marino\", \"ecosistemas marinos\", \"medio marino\",\n",
    "            \"columna de agua\", \"sedimentos marinos\",\n",
    "            \"biota marina\", \"macroalgas\", \"macrofauna bentónica\",\n",
    "            \"intermareal\", \"submareal\", \"infralitoral\",\n",
    "            \"batimetría\", \"corrientes marinas\",\n",
    "            \"amerb\", \"concesión de acuicultura\", \"concesión marítima\",\n",
    "            \"caleta\", \"pescadores artesanales\",\n",
    "            \"biodiversidad marina\", \"borde costero\",\n",
    "            \"ley 18.892\", \"lgpa\",\n",
    "            \"ds 1/1992\",\n",
    "            \"pas 111\", \"pas 112\", \"pas 113\", \"pas 114\", \"pas 115\",\n",
    "            \"pas 116\", \"pas 117\", \"pas 118\", \"pas 119\",\n",
    "            \"artículo 111\", \"artículo 112\", \"artículo 113\",\n",
    "            \"artículo 114\", \"artículo 115\", \"artículo 116\",\n",
    "            \"subpesca\", \"sernapesca\", \"directemar\", \"shoa\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"PATRIMONIO_CULTURAL\": {\n",
    "        \"nombre\": \"Patrimonio Cultural, Arqueología y Paleontología\",\n",
    "        \"keywords\": [\n",
    "            \"patrimonio cultural\", \"patrimonio arqueológico\",\n",
    "            \"arqueología\", \"arqueológico\", \"arqueológica\",\n",
    "            \"paleontología\", \"paleontológico\", \"paleontológica\",\n",
    "            \"monumento histórico\", \"monumento arqueológico\",\n",
    "            \"monumento paleontológico\", \"monumento público\",\n",
    "            \"zona típica\", \"zonas típicas\",\n",
    "            \"santuario de la naturaleza\",\n",
    "            \"consejo de monumentos nacionales\", \"cmn\",\n",
    "            \"prospección superficial\", \"pozo de sondeo\",\n",
    "            \"excavación arqueológica\", \"rescate arqueológico\",\n",
    "            \"monitoreo arqueológico\", \"hallazgo fortuito\",\n",
    "            \"artículo 26 ley 17.288\",\n",
    "            \"conformidad cmn\",\n",
    "            \"patrimonio tangible\", \"patrimonio intangible\",\n",
    "            \"ley 17.288\", \"ds 484/1990\", \"ley 21.600\",\n",
    "            \"pas 131\", \"pas 132\", \"pas 133\", \"pas 120\",\n",
    "            \"artículo 131\", \"artículo 132\", \"artículo 133\",\n",
    "            \"artículo 120\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"PAISAJE\": {\n",
    "        \"nombre\": \"Paisaje y Valor Turístico\",\n",
    "        \"keywords\": [\n",
    "            \"paisaje\", \"paisajístico\", \"valor paisajístico\",\n",
    "            \"calidad visual\", \"fragilidad visual\",\n",
    "            \"cuenca visual\", \"cuencas visuales\",\n",
    "            \"unidad de paisaje\", \"unidades de paisaje\",\n",
    "            \"punto de observación\", \"puntos de observación\",\n",
    "            \"carácter del paisaje\",\n",
    "            \"intrusión visual\", \"obstrucción visual\",\n",
    "            \"incompatibilidad visual\", \"artificialidad\",\n",
    "            \"simulación visual\", \"fotomontaje\",\n",
    "            \"valor turístico\", \"turístico\",\n",
    "            \"zoit\", \"zona de interés turístico\",\n",
    "            \"atractivo natural\", \"atractivo cultural\",\n",
    "            \"sernatur\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"AREAS_PROTEGIDAS\": {\n",
    "        \"nombre\": \"Áreas Protegidas y Sitios Prioritarios\",\n",
    "        \"keywords\": [\n",
    "            \"área protegida\", \"áreas protegidas\",\n",
    "            \"snaspe\", \"parque nacional\", \"reserva nacional\",\n",
    "            \"monumento natural\",\n",
    "            \"santuario de la naturaleza\",\n",
    "            \"sitio ramsar\",\n",
    "            \"sitio prioritario\", \"sitios prioritarios\",\n",
    "            \"amcp-mu\", \"parque marino\",\n",
    "            \"bien nacional protegido\",\n",
    "            \"acbpo\", \"área colocada bajo protección oficial\",\n",
    "            \"objeto de protección\", \"objetos de protección\",\n",
    "            \"registro nacional de áreas protegidas\",\n",
    "            \"simbio\", \"sbap\",\n",
    "            \"pas 120\", \"pas 121\", \"pas 130\",\n",
    "            \"artículo 121\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"MEDIO_HUMANO\": {\n",
    "        \"nombre\": \"Medio Humano\",\n",
    "        \"keywords\": [\n",
    "            \"medio humano\",\n",
    "            \"dimensión geográfica\", \"dimensión demográfica\",\n",
    "            \"dimensión antropológica\", \"dimensión socioeconómica\",\n",
    "            \"dimensión de bienestar social\",\n",
    "            \"grupos humanos\", \"grupo humano\",\n",
    "            \"sistemas de vida y costumbres\",\n",
    "            \"pueblo indígena\", \"pueblos indígenas\",\n",
    "            \"ghppi\", \"comunidad indígena\",\n",
    "            \"reasentamiento\", \"reasentamiento de comunidades\",\n",
    "            \"alteración significativa sistemas de vida\",\n",
    "            \"cargas ambientales\",\n",
    "            \"convenio 169\", \"oit\",\n",
    "            \"consulta indígena\",\n",
    "            \"conadi\",\n",
    "            \"percepción comunitaria\",\n",
    "            \"territorio indígena\",\n",
    "            \"actividades económicas locales\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"USO_TERRITORIO\": {\n",
    "        \"nombre\": \"Uso del Territorio y Planificación Territorial\",\n",
    "        \"keywords\": [\n",
    "            \"uso del territorio\", \"planificación territorial\",\n",
    "            \"instrumento de planificación territorial\", \"ipt\",\n",
    "            \"plan regulador comunal\", \"prc\",\n",
    "            \"plan regulador intercomunal\", \"pri\",\n",
    "            \"plan regional de ordenamiento territorial\", \"prot\",\n",
    "            \"zonificación\", \"uso de suelo\",\n",
    "            \"compatibilidad territorial\",\n",
    "            \"oguc\", \"ordenanza general\",\n",
    "            \"subdivisión predial\", \"cambio de uso de suelo\",\n",
    "            \"terreno rural\", \"límite urbano\",\n",
    "            \"pas 160\", \"pas 161\",\n",
    "            \"artículo 160\", \"artículo 161\",\n",
    "            \"calificación de instalaciones industriales\",\n",
    "            \"seremi minvu\", \"minvu\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"DESCRIPCION_PROYECTO\": {\n",
    "        \"nombre\": \"Descripción del Proyecto\",\n",
    "        \"keywords\": [\n",
    "            \"descripción del proyecto\",\n",
    "            \"partes y obras\", \"partes, obras y acciones\",\n",
    "            \"fase de construcción\", \"fase de operación\", \"fase de cierre\",\n",
    "            \"cronograma\", \"vida útil\",\n",
    "            \"mano de obra\", \"monto de inversión\",\n",
    "            \"suministros básicos\", \"insumos\",\n",
    "            \"capacidad instalada\", \"tecnología\",\n",
    "            \"layout\", \"planos\", \"coordenadas utm\",\n",
    "            \"datum wgs84\",\n",
    "            \"tipología de ingreso\", \"artículo 10\",\n",
    "            \"modificación de proyecto\", \"artículo 12\",\n",
    "            \"desarrollo por etapas\", \"artículo 14\",\n",
    "            \"inicio de ejecución\",\n",
    "            \"piscina de emergencia\", \"depósito de relave\",\n",
    "            \"botadero de estériles\", \"planta de procesos\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    # -------------------------\n",
    "    # NUEVO: ÁREA DE INFLUENCIA\n",
    "    # -------------------------\n",
    "    \"AREA_INFLUENCIA\": {\n",
    "        \"nombre\": \"Área de Influencia\",\n",
    "        \"keywords\": [\n",
    "            \"área de influencia\", \"area de influencia\",\n",
    "            \"delimitación del área de influencia\", \"delimitacion del area de influencia\",\n",
    "            \"justificación del área de influencia\", \"justificacion del area de influencia\",\n",
    "            \"criterios de delimitación\", \"criterios de delimitacion\",\n",
    "            \"polígono de área de influencia\", \"poligono de area de influencia\",\n",
    "            \"radio de influencia\",\n",
    "            \"componente en el área de influencia\", \"componente en el area de influencia\",\n",
    "            \"área de estudio\", \"area de estudio\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"RESIDUOS\": {\n",
    "        \"nombre\": \"Residuos Sólidos y Peligrosos\",\n",
    "        \"keywords\": [\n",
    "            \"residuo\", \"residuos\",\n",
    "            \"rsd\", \"residuos sólidos domiciliarios\",\n",
    "            \"rsinp\", \"residuos sólidos industriales no peligrosos\",\n",
    "            \"respel\", \"residuos peligrosos\",\n",
    "            \"reas\", \"residuos de establecimientos de atención de salud\",\n",
    "            \"riles\", \"residuos industriales líquidos\",\n",
    "            \"plan de manejo de residuos\", \"manifiesto de declaración\",\n",
    "            \"sidrep\", \"sinader\",\n",
    "            \"disposición final\", \"valorización\", \"reciclaje\",\n",
    "            \"relleno sanitario\", \"relleno de seguridad\",\n",
    "            \"bodega respel\",\n",
    "            \"toxicidad\", \"inflamabilidad\", \"corrosividad\", \"reactividad\",\n",
    "            \"ds 148/2003\", \"ds 148\",\n",
    "            \"ley 20.920\", \"ley rep\",\n",
    "            \"ds 189/2005\", \"ds 594/1999\", \"ds 594\",\n",
    "            \"pas 138\", \"pas 139\", \"pas 140\", \"pas 141\",\n",
    "            \"pas 142\", \"pas 143\", \"pas 144\", \"pas 145\",\n",
    "            \"pas 126\",\n",
    "            \"artículo 138\", \"artículo 139\", \"artículo 140\",\n",
    "            \"artículo 141\", \"artículo 142\", \"artículo 143\",\n",
    "            \"artículo 144\", \"artículo 145\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"SUSTANCIAS_PELIGROSAS\": {\n",
    "        \"nombre\": \"Sustancias Peligrosas\",\n",
    "        \"keywords\": [\n",
    "            \"sustancias peligrosas\", \"suspel\",\n",
    "            \"hoja de datos de seguridad\", \"hds\",\n",
    "            \"nch 382\", \"9 clases\",\n",
    "            \"cubeto de contención\", \"contención secundaria\",\n",
    "            \"compatibilidad química\", \"segregación\",\n",
    "            \"bodega de sustancias peligrosas\",\n",
    "            \"transporte de cargas peligrosas\",\n",
    "            \"ds 43/2015\", \"ds 78/2009\",\n",
    "            \"nch 2190\", \"nch 2120\",\n",
    "            \"ds 298/1994\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"TRANSPORTE_VIALIDAD\": {\n",
    "        \"nombre\": \"Transporte y Vialidad\",\n",
    "        \"keywords\": [\n",
    "            \"transporte\", \"vialidad\",\n",
    "            \"impacto vial\", \"estudio de impacto vial\",\n",
    "            \"imiv\", \"eistu\",\n",
    "            \"generación de viajes\", \"atracción de viajes\",\n",
    "            \"nivel de servicio\", \"capacidad vial\",\n",
    "            \"flujo vehicular\", \"flujos vehiculares\",\n",
    "            \"intersección\", \"intersecciones\",\n",
    "            \"sectra\", \"uoct\", \"seremitt\",\n",
    "            \"saturn\", \"transyt\", \"aimsun\", \"estraus\",\n",
    "            \"vivaldi\", \"modem\", \"modec\",\n",
    "            \"libre circulación\", \"conectividad\",\n",
    "            \"tiempos de desplazamiento\",\n",
    "            \"ley 20.958\", \"ds 30/2017\",\n",
    "            \"ds 83/1985\", \"dfl 850\",\n",
    "            \"ley de caminos\",\n",
    "            \"dirección de vialidad\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"PLAN_MEDIDAS\": {\n",
    "        \"nombre\": \"Plan de Medidas de Mitigación, Reparación y Compensación\",\n",
    "        \"keywords\": [\n",
    "            \"plan de medidas\", \"medida de mitigación\",\n",
    "            \"medidas de mitigación\", \"medida de reparación\",\n",
    "            \"medidas de reparación\", \"medida de compensación\",\n",
    "            \"medidas de compensación\",\n",
    "            \"compromiso ambiental voluntario\",\n",
    "            \"compensación de biodiversidad\",\n",
    "            \"plan de rescate\", \"plan de revegetación\",\n",
    "            \"plan de reforestación\",\n",
    "            \"equivalencia ecológica\", \"adicionalidad\",\n",
    "            \"no pérdida neta\", \"permanencia\",\n",
    "            \"artículo 97\", \"artículo 98\", \"artículo 99\",\n",
    "            \"artículo 100\", \"artículo 101\", \"artículo 102\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"PLAN_CONTINGENCIAS\": {\n",
    "        \"nombre\": \"Plan de Contingencias y Emergencias\",\n",
    "        \"keywords\": [\n",
    "            \"plan de contingencia\", \"plan de emergencia\",\n",
    "            \"contingencias y emergencias\",\n",
    "            \"prevención de contingencias\",\n",
    "            \"riesgo ambiental\", \"riesgos ambientales\",\n",
    "            \"derrame\", \"incendio\", \"explosión\", \"fuga\",\n",
    "            \"protocolo de actuación\", \"simulacro\",\n",
    "            \"sistema de alerta temprana\",\n",
    "            \"matriz de riesgos\",\n",
    "            \"artículo 103\", \"artículo 104\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"PLAN_SEGUIMIENTO\": {\n",
    "        \"nombre\": \"Plan de Seguimiento de Variables Ambientales\",\n",
    "        \"keywords\": [\n",
    "            \"plan de seguimiento\", \"seguimiento ambiental\",\n",
    "            \"monitoreo ambiental\", \"variable de seguimiento\",\n",
    "            \"frecuencia de muestreo\", \"punto de monitoreo\",\n",
    "            \"puntos de monitoreo\",\n",
    "            \"umbrales de acción\",\n",
    "            \"informe de seguimiento\",\n",
    "            \"monitoreo participativo\",\n",
    "            \"verificación de cumplimiento\",\n",
    "            \"efectividad de medidas\",\n",
    "            \"artículo 105\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"NORMATIVA_AMBIENTAL\": {\n",
    "        \"nombre\": \"Legislación y Normativa Ambiental Aplicable\",\n",
    "        \"keywords\": [\n",
    "            \"normativa ambiental aplicable\",\n",
    "            \"legislación ambiental\",\n",
    "            \"norma de emisión\", \"normas de emisión\",\n",
    "            \"norma de calidad\", \"normas de calidad\",\n",
    "            \"norma primaria\", \"norma secundaria\",\n",
    "            \"plan de cumplimiento\",\n",
    "            \"norma de referencia\", \"normas de referencia\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"PARTICIPACION_CIUDADANA\": {\n",
    "        \"nombre\": \"Participación Ciudadana y Consulta Indígena\",\n",
    "        \"keywords\": [\n",
    "            \"participación ciudadana\", \"pac\",\n",
    "            \"observaciones ciudadanas\",\n",
    "            \"respuesta a observaciones\",\n",
    "            \"consulta indígena\",\n",
    "            \"convenio 169\", \"oit\",\n",
    "            \"participación ciudadana temprana\",\n",
    "            \"ponderación de observaciones\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"CAMBIO_CLIMATICO\": {\n",
    "        \"nombre\": \"Cambio Climático y Gases de Efecto Invernadero\",\n",
    "        \"keywords\": [\n",
    "            \"cambio climático\",\n",
    "            \"gases de efecto invernadero\", \"gei\",\n",
    "            \"co2\", \"co₂\", \"ch4\", \"ch₄\", \"n2o\", \"n₂o\",\n",
    "            \"huella de carbono\",\n",
    "            \"carbono negro\", \"forzantes climáticos\",\n",
    "            \"adaptación al cambio climático\",\n",
    "            \"vulnerabilidad climática\",\n",
    "            \"escenario rcp\", \"escenario ssp\",\n",
    "            \"ley 21.455\", \"ley marco cambio climático\",\n",
    "            \"ndc chile\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"RIESGO_SALUD\": {\n",
    "        \"nombre\": \"Riesgo para la Salud de la Población\",\n",
    "        \"keywords\": [\n",
    "            \"riesgo para la salud\", \"riesgo en salud\",\n",
    "            \"evaluación de riesgo en salud\",\n",
    "            \"vía de exposición\", \"vías de exposición\",\n",
    "            \"inhalación\", \"ingestión\", \"contacto dérmico\",\n",
    "            \"población susceptible\", \"poblaciones susceptibles\",\n",
    "            \"radiación electromagnética\",\n",
    "            \"vectores sanitarios\",\n",
    "            \"epidemiología\",\n",
    "            \"artículo 5 rseia\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    \"MINERIA\": {\n",
    "        \"nombre\": \"Aspectos Mineros\",\n",
    "        \"keywords\": [\n",
    "            \"minería\", \"minero\", \"minera\",\n",
    "            \"depósito de relave\", \"relaves\", \"relave\",\n",
    "            \"relave en pasta\", \"relave espesado\",\n",
    "            \"botadero de estériles\", \"estériles\",\n",
    "            \"plan de cierre de faena\", \"cierre de faena minera\",\n",
    "            \"drenaje ácido\", \"dam\", \"aguas ácidas\",\n",
    "            \"mineral\", \"concentrado\", \"lixiviación\",\n",
    "            \"chancado\", \"molienda\", \"flotación\",\n",
    "            \"pila de lixiviación\",\n",
    "            \"pas 135\", \"pas 136\", \"pas 137\",\n",
    "            \"pas 121\", \"pas 122\", \"pas 125\", \"pas 154\",\n",
    "            \"artículo 135\", \"artículo 136\", \"artículo 137\",\n",
    "            \"sernageomin\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    # -------------------------\n",
    "    # NUEVO: PAS\n",
    "    # -------------------------\n",
    "    \"PAS\": {\n",
    "        \"nombre\": \"PAS (Permisos Ambientales Sectoriales)\",\n",
    "        \"keywords\": [\n",
    "            \"pas\", \"permiso ambiental sectorial\", \"permisos ambientales sectoriales\",\n",
    "            # Artículos (variantes)\n",
    "            \"artículo 111\", \"articulo 111\", \"art. 111\",\n",
    "            \"artículo 112\", \"articulo 112\", \"art. 112\",\n",
    "            \"artículo 113\", \"articulo 113\", \"art. 113\",\n",
    "            \"artículo 114\", \"articulo 114\", \"art. 114\",\n",
    "            \"artículo 115\", \"articulo 115\", \"art. 115\",\n",
    "            \"artículo 116\", \"articulo 116\", \"art. 116\",\n",
    "            \"artículo 117\", \"articulo 117\", \"art. 117\",\n",
    "            \"artículo 118\", \"articulo 118\", \"art. 118\",\n",
    "            \"artículo 119\", \"articulo 119\", \"art. 119\",\n",
    "            \"artículo 120\", \"articulo 120\", \"art. 120\",\n",
    "            \"artículo 121\", \"articulo 121\", \"art. 121\",\n",
    "            \"artículo 122\", \"articulo 122\", \"art. 122\",\n",
    "            \"artículo 123\", \"articulo 123\", \"art. 123\",\n",
    "            \"artículo 124\", \"articulo 124\", \"art. 124\",\n",
    "            \"artículo 125\", \"articulo 125\", \"art. 125\",\n",
    "            \"artículo 126\", \"articulo 126\", \"art. 126\",\n",
    "            \"artículo 127\", \"articulo 127\", \"art. 127\",\n",
    "            \"artículo 128\", \"articulo 128\", \"art. 128\",\n",
    "            \"artículo 129\", \"articulo 129\", \"art. 129\",\n",
    "            \"artículo 130\", \"articulo 130\", \"art. 130\",\n",
    "            \"artículo 131\", \"articulo 131\", \"art. 131\",\n",
    "            \"artículo 132\", \"articulo 132\", \"art. 132\",\n",
    "            \"artículo 133\", \"articulo 133\", \"art. 133\",\n",
    "            \"artículo 134\", \"articulo 134\", \"art. 134\",\n",
    "            \"artículo 135\", \"articulo 135\", \"art. 135\",\n",
    "            \"artículo 136\", \"articulo 136\", \"art. 136\",\n",
    "            \"artículo 137\", \"articulo 137\", \"art. 137\",\n",
    "            \"artículo 138\", \"articulo 138\", \"art. 138\",\n",
    "            \"artículo 139\", \"articulo 139\", \"art. 139\",\n",
    "            \"artículo 140\", \"articulo 140\", \"art. 140\",\n",
    "            \"artículo 141\", \"articulo 141\", \"art. 141\",\n",
    "            \"artículo 142\", \"articulo 142\", \"art. 142\",\n",
    "            \"artículo 143\", \"articulo 143\", \"art. 143\",\n",
    "            \"artículo 144\", \"articulo 144\", \"art. 144\",\n",
    "            \"artículo 145\", \"articulo 145\", \"art. 145\",\n",
    "            \"artículo 146\", \"articulo 146\", \"art. 146\",\n",
    "            \"artículo 147\", \"articulo 147\", \"art. 147\",\n",
    "            \"artículo 148\", \"articulo 148\", \"art. 148\",\n",
    "            \"artículo 149\", \"articulo 149\", \"art. 149\",\n",
    "            \"artículo 150\", \"articulo 150\", \"art. 150\",\n",
    "            \"artículo 151\", \"articulo 151\", \"art. 151\",\n",
    "            \"artículo 152\", \"articulo 152\", \"art. 152\",\n",
    "            \"artículo 153\", \"articulo 153\", \"art. 153\",\n",
    "            \"artículo 154\", \"articulo 154\", \"art. 154\",\n",
    "            \"artículo 155\", \"articulo 155\", \"art. 155\",\n",
    "            \"artículo 156\", \"articulo 156\", \"art. 156\",\n",
    "            \"artículo 157\", \"articulo 157\", \"art. 157\",\n",
    "            \"artículo 158\", \"articulo 158\", \"art. 158\",\n",
    "            \"artículo 159\", \"articulo 159\", \"art. 159\",\n",
    "            \"artículo 160\", \"articulo 160\", \"art. 160\",\n",
    "            \"artículo 161\", \"articulo 161\", \"art. 161\",\n",
    "            # PAS n\n",
    "            \"pas 111\", \"pas 112\", \"pas 113\", \"pas 114\", \"pas 115\", \"pas 116\",\n",
    "            \"pas 117\", \"pas 118\", \"pas 119\", \"pas 120\", \"pas 121\",\n",
    "            \"pas 122\", \"pas 123\", \"pas 124\", \"pas 125\", \"pas 126\",\n",
    "            \"pas 127\", \"pas 128\", \"pas 129\", \"pas 130\",\n",
    "            \"pas 131\", \"pas 132\", \"pas 133\", \"pas 134\",\n",
    "            \"pas 135\", \"pas 136\", \"pas 137\",\n",
    "            \"pas 138\", \"pas 139\", \"pas 140\", \"pas 141\", \"pas 142\", \"pas 143\",\n",
    "            \"pas 144\", \"pas 145\", \"pas 146\", \"pas 147\", \"pas 148\", \"pas 149\",\n",
    "            \"pas 150\", \"pas 151\", \"pas 152\", \"pas 153\", \"pas 154\",\n",
    "            \"pas 155\", \"pas 156\", \"pas 157\", \"pas 158\", \"pas 159\",\n",
    "            \"pas 160\", \"pas 161\",\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# NORMALIZACIÓN DE TEXTO\n",
    "# =============================================================================\n",
    "def normalizar(texto: str) -> str:\n",
    "    \"\"\"Normaliza texto para matching: minúsculas, espacios.\"\"\"\n",
    "    if not texto:\n",
    "        return \"\"\n",
    "    t = texto.lower().strip()\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "\n",
    "def normalizar_sin_acentos(texto: str) -> str:\n",
    "    \"\"\"Normalización sin acentos para matching más flexible.\"\"\"\n",
    "    t = normalizar(texto)\n",
    "    repl = {\"á\": \"a\", \"é\": \"e\", \"í\": \"i\", \"ó\": \"o\", \"ú\": \"u\", \"ü\": \"u\", \"ñ\": \"n\"}\n",
    "    for old, new in repl.items():\n",
    "        t = t.replace(old, new)\n",
    "    return t\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MOTOR DE CLASIFICACIÓN\n",
    "# =============================================================================\n",
    "def calcular_score_tema(\n",
    "    tema_id: str,\n",
    "    tema_data: dict,\n",
    "    cap_norm: str,\n",
    "    bis_norm: str,\n",
    "    txt_norm: str,\n",
    "    cap_sin: str,\n",
    "    bis_sin: str,\n",
    "    txt_sin: str,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Calcula el score de un tema para una pregunta.\n",
    "    Retorna: {\"score\": float, \"matches\": [str], \"detalle\": {...}}\n",
    "    \"\"\"\n",
    "    keywords = tema_data[\"keywords\"]\n",
    "    matches = []\n",
    "    score = 0.0\n",
    "    detalle = {\"capitulo\": [], \"bisagra\": [], \"texto\": []}\n",
    "\n",
    "    for kw in keywords:\n",
    "        kw_norm = normalizar(kw)\n",
    "        kw_sin = normalizar_sin_acentos(kw)\n",
    "\n",
    "        # Keywords cortos: palabra completa\n",
    "        if len(kw_norm) <= 4:\n",
    "            pattern = r\"\\b\" + re.escape(kw_norm) + r\"\\b\"\n",
    "            pattern_sin = r\"\\b\" + re.escape(kw_sin) + r\"\\b\"\n",
    "        else:\n",
    "            pattern = re.escape(kw_norm)\n",
    "            pattern_sin = re.escape(kw_sin)\n",
    "\n",
    "        found = False\n",
    "\n",
    "        if cap_norm and (re.search(pattern, cap_norm) or re.search(pattern_sin, cap_sin)):\n",
    "            score += PESO_CAPITULO\n",
    "            detalle[\"capitulo\"].append(kw)\n",
    "            found = True\n",
    "\n",
    "        if bis_norm and (re.search(pattern, bis_norm) or re.search(pattern_sin, bis_sin)):\n",
    "            score += PESO_BISAGRA\n",
    "            detalle[\"bisagra\"].append(kw)\n",
    "            found = True\n",
    "\n",
    "        if txt_norm and (re.search(pattern, txt_norm) or re.search(pattern_sin, txt_sin)):\n",
    "            score += PESO_TEXTO\n",
    "            detalle[\"texto\"].append(kw)\n",
    "            found = True\n",
    "\n",
    "        if found:\n",
    "            matches.append(kw)\n",
    "\n",
    "    return {\"score\": score, \"matches\": matches, \"detalle\": detalle}\n",
    "\n",
    "\n",
    "def clasificar_pregunta(pregunta: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Clasifica una pregunta en temas ICSARA.\n",
    "    Retorna:\n",
    "      - tema_principal / tema_principal_id (compatibilidad: primer principal)\n",
    "      - temas_principales / temas_principales_id (lista)\n",
    "      - temas (lista completa ordenada con detalle)\n",
    "    \"\"\"\n",
    "    cap = pregunta.get(\"capitulo\", \"\") or \"\"\n",
    "    bis = pregunta.get(\"bisagra\", \"\") or \"\"\n",
    "    txt = pregunta.get(\"texto\", \"\") or \"\"\n",
    "\n",
    "    cap_norm = normalizar(cap)\n",
    "    bis_norm = normalizar(bis)\n",
    "    txt_norm = normalizar(txt)\n",
    "\n",
    "    cap_sin = normalizar_sin_acentos(cap)\n",
    "    bis_sin = normalizar_sin_acentos(bis)\n",
    "    txt_sin = normalizar_sin_acentos(txt)\n",
    "\n",
    "    resultados = []\n",
    "    for tema_id, tema_data in TAXONOMIA.items():\n",
    "        r = calcular_score_tema(\n",
    "            tema_id, tema_data,\n",
    "            cap_norm, bis_norm, txt_norm,\n",
    "            cap_sin, bis_sin, txt_sin,\n",
    "        )\n",
    "        if r[\"score\"] >= MIN_SCORE:\n",
    "            resultados.append({\n",
    "                \"id\": tema_id,\n",
    "                \"nombre\": tema_data[\"nombre\"],\n",
    "                \"score\": r[\"score\"],\n",
    "                \"matches\": r[\"matches\"],\n",
    "                \"detalle\": r[\"detalle\"],\n",
    "            })\n",
    "\n",
    "    resultados.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "    if not resultados:\n",
    "        return {\n",
    "            \"tema_principal\": \"Sin clasificar\",\n",
    "            \"tema_principal_id\": \"SIN_CLASIFICAR\",\n",
    "            \"temas_principales\": [],\n",
    "            \"temas_principales_id\": [],\n",
    "            \"temas\": [],\n",
    "        }\n",
    "\n",
    "    top_score = resultados[0][\"score\"]\n",
    "    thr_principal = max(MIN_SCORE, MULTI_PRINCIPAL_RATIO * top_score)\n",
    "\n",
    "    principales = [t for t in resultados if t[\"score\"] >= thr_principal]\n",
    "\n",
    "    return {\n",
    "        \"tema_principal\": principales[0][\"nombre\"],\n",
    "        \"tema_principal_id\": principales[0][\"id\"],\n",
    "        \"temas_principales\": [t[\"nombre\"] for t in principales],\n",
    "        \"temas_principales_id\": [t[\"id\"] for t in principales],\n",
    "        \"temas\": resultados,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN\n",
    "# =============================================================================\n",
    "def main():\n",
    "    input_path = INPUT_JSON\n",
    "    if not input_path.exists():\n",
    "        print(f\"No se encontro: {input_path}\")\n",
    "        return\n",
    "\n",
    "    preguntas = json.loads(input_path.read_text(encoding=\"utf-8\"))\n",
    "    print(f\"Preguntas cargadas: {len(preguntas)}\")\n",
    "    print(f\"Temas disponibles: {len(TAXONOMIA)}\")\n",
    "    print(f\"Multi-principal ratio: {MULTI_PRINCIPAL_RATIO:.2f}\")\n",
    "\n",
    "    resultados_out = []\n",
    "    resultados_detalle_out = []\n",
    "    for p in preguntas:\n",
    "        clf = clasificar_pregunta(p)\n",
    "\n",
    "        # matches del tema top (para no saturar)\n",
    "        top_matches = clf[\"temas\"][0][\"matches\"][:10] if clf[\"temas\"] else []\n",
    "\n",
    "        # secundarios: top temas que NO quedaron en principales\n",
    "        principales_set = set(clf.get(\"temas_principales\", []))\n",
    "        secundarios = [t for t in clf.get(\"temas\", []) if t[\"nombre\"] not in principales_set]\n",
    "\n",
    "        resultados_out.append({\n",
    "            \"numero\": p.get(\"numero\"),\n",
    "            \"capitulo\": p.get(\"capitulo\", \"\"),\n",
    "            \"bisagra\": p.get(\"bisagra\"),\n",
    "            \"tema_principal\": clf[\"tema_principal\"],                 # compat\n",
    "            \"tema_principal_id\": clf[\"tema_principal_id\"],           # compat\n",
    "            \"temas_principales\": clf.get(\"temas_principales\", []),   # nuevo\n",
    "            \"temas_principales_id\": clf.get(\"temas_principales_id\", []),  # nuevo\n",
    "            \"score\": clf[\"temas\"][0][\"score\"] if clf[\"temas\"] else 0,\n",
    "            \"temas_secundarios\": [\n",
    "                {\"nombre\": t[\"nombre\"], \"score\": t[\"score\"]}\n",
    "                for t in secundarios[:3]\n",
    "            ],\n",
    "            \"keywords_match\": top_matches,\n",
    "            \"texto\": p.get(\"texto\", \"\"),\n",
    "            \"tablas_figuras\": p.get(\"tablas_figuras\", []),\n",
    "        })\n",
    "\n",
    "        resultados_detalle_out.append({\n",
    "            \"numero\": p.get(\"numero\"),\n",
    "            \"capitulo\": p.get(\"capitulo\", \"\"),\n",
    "            \"bisagra\": p.get(\"bisagra\"),\n",
    "            \"tema_principal\": clf[\"tema_principal\"],\n",
    "            \"tema_principal_id\": clf[\"tema_principal_id\"],\n",
    "            \"temas_principales\": clf.get(\"temas_principales\", []),\n",
    "            \"temas_principales_id\": clf.get(\"temas_principales_id\", []),\n",
    "            \"temas\": clf.get(\"temas\", []),\n",
    "        })\n",
    "\n",
    "    output_path = OUTPUT_JSON\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_path.write_text(\n",
    "        json.dumps(resultados_out, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    output_detalle_path = OUTPUT_JSON_DETALLE\n",
    "    output_detalle_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    output_detalle_path.write_text(\n",
    "        json.dumps(resultados_detalle_out, ensure_ascii=False, indent=2),\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "\n",
    "    # === REPORTE ===\n",
    "    print(f\"\\nSalida: {output_path}\")\n",
    "    print(f\"Salida detalle: {output_detalle_path}\")\n",
    "\n",
    "    dist = Counter(r[\"tema_principal\"] for r in resultados_out)\n",
    "    n_sin = dist.get(\"Sin clasificar\", 0)\n",
    "\n",
    "    print(f\"\\nClasificadas: {len(resultados_out) - n_sin}/{len(resultados_out)}\")\n",
    "    if n_sin:\n",
    "        print(f\"Sin clasificar: {n_sin}\")\n",
    "\n",
    "    print(f\"\\nDistribución temática (por primer principal):\")\n",
    "    print(\"-\" * 65)\n",
    "    for tema, count in dist.most_common():\n",
    "        bar = \"#\" * min(count, 40)\n",
    "        print(f\"  {tema[:45]:<45} {count:>3}  {bar}\")\n",
    "\n",
    "    print(f\"\\nEjemplos (primeras 10):\")\n",
    "    print(\"-\" * 65)\n",
    "    for r in resultados_out[:10]:\n",
    "        prims = \", \".join(r.get(\"temas_principales\", [])[:3])\n",
    "        sec = \", \".join(t[\"nombre\"][:30] for t in (r.get(\"temas_secundarios\") or [])[:2])\n",
    "        kws = \", \".join((r.get(\"keywords_match\") or [])[:4])\n",
    "\n",
    "        print(f\"  P{r['numero']!s:>3}: {r['tema_principal'][:40]:<40} (score:{r['score']:.0f})\")\n",
    "        if prims:\n",
    "            print(f\"        Principales: {prims}\")\n",
    "        if sec:\n",
    "            print(f\"        Secundarios: {sec}\")\n",
    "        if kws:\n",
    "            print(f\"        Keywords: {kws}\")\n",
    "\n",
    "    sin_clf = [r[\"numero\"] for r in resultados_out if r[\"tema_principal\"] == \"Sin clasificar\"]\n",
    "    if sin_clf:\n",
    "        print(f\"\\nPreguntas sin clasificar: {sin_clf}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
